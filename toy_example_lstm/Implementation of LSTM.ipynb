{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of Long short-term memory (LSTM) \n",
    "# module with numpy\n",
    "\n",
    "\n",
    "<img src=\"lstm1.png\" width=\"500px\">\n",
    "\n",
    "This implementation is based on materials from [[1](https://iamtrask.github.io/2015/11/15/anyone-can-code-lstm/)]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1./(1 + np.exp(-z))\n",
    "\n",
    "def tanh(z):\n",
    "    return np.tanh(z)\n",
    "\n",
    "def softmax(z):\n",
    "    s = np.exp(z)/( np.sum( np.exp(z) ) )\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3.4379513 ],\n",
       "       [-0.50261336],\n",
       "       [ 1.33089875]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.randn(3, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of x_t: (1, 3) | h_prev: (1, 2) | c_prev: (1, 2)\n",
      "Shape of concat_input: (1, 5)\n"
     ]
    }
   ],
   "source": [
    "n_x_features = 3 #Number of features in input vector\n",
    "n_inputs = 1 #number of input examples\n",
    "n_hidden_units = 2\n",
    "n_output_units = 3\n",
    "# input at step t\n",
    "x_t = np.random.randn(n_inputs, n_x_features)\n",
    "# previous hidden layer output h\n",
    "h_prev = np.random.randn( n_inputs, n_hidden_units )\n",
    "#previous state same shape as prev hidden layer\n",
    "c_prev = np.random.randn( *h_prev.shape )\n",
    "concat_input = np.concatenate((h_prev, x_t), axis=1) #\n",
    "\n",
    "print(\"Shape of x_t: {} | h_prev: {} | c_prev: {}\".format(x_t.shape, h_prev.shape, c_prev.shape) )\n",
    "print(\"Shape of concat_input: {}\".format(concat_input.shape) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forget Gate\n",
    "\n",
    "![](lstm2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forget weight shape: (5, 2)\n"
     ]
    }
   ],
   "source": [
    "bias_f = np.ones([n_hidden_units])\n",
    "weight_f = np.random.normal(loc=0.0, scale=0.1, size=(concat_input.shape[1], n_hidden_units) )\n",
    "print(\"Forget weight shape: {}\".format(weight_f.shape) )\n",
    "#concat x and h\n",
    "z_f = np.dot(concat_input, weight_f) + bias_f\n",
    "f_t = sigmoid( z_f )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute state at step t\n",
    "\n",
    "\n",
    "![](lstm3.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State at time t: [[ 0.62436412  0.09535652]]\n"
     ]
    }
   ],
   "source": [
    "weight_i = np.random.normal(loc=0.0, scale=0.1, size=(concat_input.shape[1], n_hidden_units) )\n",
    "weight_ctild = np.random.normal(loc=0.0, scale=0.1, size=(concat_input.shape[1], n_hidden_units) )\n",
    "bias_i = np.ones( [n_hidden_units] )\n",
    "bias_ctild = np.ones( [n_hidden_units] )\n",
    "z_i = np.dot( concat_input, weight_i ) + bias_i\n",
    "z_ctild = np.dot( concat_input, weight_ctild ) + bias_ctild\n",
    "\n",
    "#apply activation\n",
    "i_t = sigmoid(z_i)\n",
    "ctild_t = tanh( z_ctild )\n",
    "\n",
    "#state at time step t\n",
    "c_t = f_t * c_prev + i_t * ctild_t\n",
    "print(\"State at time t: {}\".format(c_t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute hidden Output\n",
    "\n",
    "![](lstm4.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weight_o = np.random.normal(loc=0.0, scale=0.1, size=(concat_input.shape[1], n_hidden_units) )\n",
    "bias_o = np.ones( [n_hidden_units] )\n",
    "z_o = np.dot( concat_input, weight_o ) + bias_o\n",
    "o_t = sigmoid( z_o )\n",
    "h_t = o_t * tanh(c_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute final output using h_t (FCN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weight_final = np.random.normal(loc=0.0, scale=0.1, size=(n_hidden_units, n_output_units) )\n",
    "bias_output = np.ones([n_output_units])\n",
    "z_final = np.dot(h_t, weight_final) + bias_output\n",
    "output_t = softmax(z_final)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation through time (BPTT)# Concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
